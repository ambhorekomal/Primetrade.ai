{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T0kQC9i6BkPx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import statsmodels.formula.api as smf\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HIST_PATH = \"historical_data.csv\"\n",
        "FG_PATH   = \"fear_greed_index.csv\"\n",
        "OUT_DIR   = \"outputs\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "LxaGPZ8jB6Jq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_cols(df):\n",
        "    # make column names simple: lowercase, underscores\n",
        "    df = df.copy()\n",
        "    newcols = {}\n",
        "    for c in df.columns:\n",
        "        cn = c.strip().lower().replace(' ', '_').replace('.', '').replace('/', '_')\n",
        "        newcols[c] = cn\n",
        "    return df.rename(columns=newcols)\n",
        "\n",
        "def detect_time_series_units(series):\n",
        "    # returns 'ms', 's' or None\n",
        "    vals = series.dropna().astype(float)\n",
        "    if len(vals) == 0:\n",
        "        return None\n",
        "    med = np.median(vals)\n",
        "    # heuristics:\n",
        "    if med > 1e12:\n",
        "        return 'ms'\n",
        "    if med > 1e9:\n",
        "        return 's'\n",
        "    return None"
      ],
      "metadata": {
        "id": "e1RolybfB9mU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = pd.read_csv(HIST_PATH)\n",
        "fg   = pd.read_csv(FG_PATH)\n",
        "\n",
        "hist = normalize_cols(hist)\n",
        "fg   = normalize_cols(fg)\n",
        "\n",
        "print(\"Historical columns:\", hist.columns.tolist())\n",
        "print(\"FG columns:\", fg.columns.tolist())\n",
        "\n",
        "# Save initial samples\n",
        "hist.head(5).to_csv(os.path.join(OUT_DIR,\"hist_head.csv\"), index=False)\n",
        "fg.head(5).to_csv(os.path.join(OUT_DIR,\"fg_head.csv\"), index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHgjDdh3CBol",
        "outputId": "ecb25624-ce6f-4b94-fb63-a489b57d88b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Historical columns: ['account', 'coin', 'execution_price', 'size_tokens', 'size_usd', 'side', 'timestamp_ist', 'start_position', 'direction', 'closed_pnl', 'transaction_hash', 'order_id', 'crossed', 'fee', 'trade_id', 'timestamp']\n",
            "FG columns: ['timestamp', 'value', 'classification', 'date']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Historical timestamp: common names 'timestamp', 'time', 'date', 'datetime'\n",
        "hist_time_col = None\n",
        "for c in ['timestamp','time','date','datetime']:\n",
        "    if c in hist.columns:\n",
        "        hist_time_col = c\n",
        "        break\n",
        "\n"
      ],
      "metadata": {
        "id": "I_WCsBU6CFHK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FG: based on your file the date column is 'date' (string like '2018-02-01')\n",
        "fg_date_col = 'date' if 'date' in fg.columns else None\n",
        "\n",
        "# Parse HIST timestamp: detect ms/s or ISO\n",
        "if hist_time_col:\n",
        "    unit = detect_time_series_units(hist[hist_time_col])\n",
        "    if unit == 'ms':\n",
        "        hist['parsed_time'] = pd.to_datetime(hist[hist_time_col].astype(float), unit='ms', errors='coerce')\n",
        "        print(\"Parsed historical timestamps as milliseconds (unit='ms').\")\n",
        "    elif unit == 's':\n",
        "        hist['parsed_time'] = pd.to_datetime(hist[hist_time_col].astype(float), unit='s', errors='coerce')\n",
        "        print(\"Parsed historical timestamps as seconds (unit='s').\")\n",
        "    else:\n",
        "        # fallback: try direct parse (ISO or already readable)\n",
        "        hist['parsed_time'] = pd.to_datetime(hist[hist_time_col], errors='coerce')\n",
        "        print(\"Parsed historical timestamps using pd.to_datetime (fallback).\")\n",
        "else:\n",
        "    hist['parsed_time'] = pd.NaT\n",
        "    print(\"No timestamp-like column detected in historical data; 'parsed_time' is NaT.\")\n",
        "\n",
        "# Parse FG date column directly as a calendar date string (FIXED)\n",
        "if fg_date_col:\n",
        "    fg['parsed_date'] = pd.to_datetime(fg[fg_date_col], errors='coerce')   # <-- SIMPLE direct parse (no astype(float))\n",
        "    print(\"Parsed FG 'date' column using pd.to_datetime.\")\n",
        "else:\n",
        "    fg['parsed_date'] = pd.NaT\n",
        "    print(\"No FG date column detected; 'parsed_date' is NaT.\")\n",
        "\n",
        "# Create date keys for daily join\n",
        "hist['trade_date'] = hist['parsed_time'].dt.date\n",
        "fg['fg_date'] = fg['parsed_date'].dt.date\n",
        "\n",
        "print(\"Historical time span:\", hist['parsed_time'].min(), \"to\", hist['parsed_time'].max())\n",
        "print(\"FG time span:\", fg['parsed_date'].min(), \"to\", fg['parsed_date'].max())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsj_trx2CK_s",
        "outputId": "5ba56a87-7679-495e-909b-7e7a9b797395"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed historical timestamps as milliseconds (unit='ms').\n",
            "Parsed FG 'date' column using pd.to_datetime.\n",
            "Historical time span: 2023-03-28 10:40:00 to 2025-06-15 15:06:40\n",
            "FG time span: 2018-02-01 00:00:00 to 2025-05-02 00:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pick(cols, candidates):\n",
        "    for c in candidates:\n",
        "        if c in cols:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "pnl_col    = pick(hist.columns, ['closed_pnl','closedpnl','pnl','profit','realized_pnl','realised_pnl','closed_pnl_usd'])\n",
        "size_col   = pick(hist.columns, ['size_usd','size','size_tokens','size_usd...on_hash','size_usd'])\n",
        "price_col  = pick(hist.columns, ['execution_price','executionprice','price'])\n",
        "side_col   = pick(hist.columns, ['side','direction'])\n",
        "account_col= pick(hist.columns, ['account','user','client'])\n",
        "symbol_col = pick(hist.columns, ['coin','symbol','pair'])\n",
        "leverage_col = pick(hist.columns, ['leverage','lev'])\n",
        "\n",
        "print(\"Detected mapping:\")\n",
        "print(\"pnl:\", pnl_col, \"size:\", size_col, \"price:\", price_col, \"side:\", side_col)\n",
        "print(\"account:\", account_col, \"symbol:\", symbol_col, \"leverage:\", leverage_col)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4qaurW4D-y3",
        "outputId": "8a7f369e-7b7e-417d-8092-620fd12078be"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected mapping:\n",
            "pnl: closed_pnl size: size_usd price: execution_price side: side\n",
            "account: account symbol: coin leverage: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FG: if 'value' numeric exists use it; else map classification strings to ordinal\n",
        "if 'value' in fg.columns:\n",
        "    fg['fg_index'] = pd.to_numeric(fg['value'], errors='coerce')\n",
        "else:\n",
        "    # map common labels (\"Extreme Fear\", \"Fear\", \"Neutral\", \"Greed\", \"Extreme Greed\") to numeric\n",
        "    if 'classification' in fg.columns:\n",
        "        def map_label(x):\n",
        "            s = str(x).lower()\n",
        "            if 'extreme fear' in s: return 10.0\n",
        "            if 'fear' in s: return 25.0\n",
        "            if 'neutral' in s: return 50.0\n",
        "            if 'extreme greed' in s: return 90.0\n",
        "            if 'greed' in s: return 75.0\n",
        "            try:\n",
        "                return float(x)\n",
        "            except:\n",
        "                return np.nan\n",
        "        fg['fg_index'] = fg['classification'].apply(map_label)\n",
        "    else:\n",
        "        fg['fg_index'] = np.nan\n",
        "\n",
        "# Merge hist (trades) with FG on calendar date (left join)\n",
        "fg_merge_cols = ['fg_date']\n",
        "if 'classification' in fg.columns:\n",
        "    fg_merge_cols.append('classification')\n",
        "fg_merge_cols.append('fg_index')\n",
        "\n",
        "fg_small = fg[fg_merge_cols].drop_duplicates()\n",
        "merged = hist.merge(fg_small, left_on='trade_date', right_on='fg_date', how='left')\n",
        "print(\"Merged shape:\", merged.shape)\n",
        "\n",
        "# Save merged quick\n",
        "merged.to_csv(os.path.join(OUT_DIR, \"cleaned_merged_raw.csv\"), index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ey7h7uMEDVz",
        "outputId": "c3bd7bb9-c89c-4482-ecf4-976a9cc4df72"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged shape: (211224, 21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Closed PnL numeric\n",
        "if pnl_col and pnl_col in merged.columns:\n",
        "    merged['closed_pnl'] = pd.to_numeric(merged[pnl_col], errors='coerce')\n",
        "else:\n",
        "    merged['closed_pnl'] = np.nan\n",
        "    print(\"Warning: closed PnL column not found; 'closed_pnl' set to NaN. If you have a PnL column, rename it to one of:\",\n",
        "          \"closed_pnl, closedpnl, pnl, profit, realized_pnl, realised_pnl\")\n",
        "\n",
        "# Notional exposure (prefer size_usd)\n",
        "if 'size_usd' in merged.columns and merged['size_usd'].notna().sum() > 0:\n",
        "    merged['notional'] = pd.to_numeric(merged['size_usd'], errors='coerce')\n",
        "elif size_col and price_col and size_col in merged.columns and price_col in merged.columns:\n",
        "    merged['notional'] = pd.to_numeric(merged[size_col], errors='coerce') * pd.to_numeric(merged[price_col], errors='coerce')\n",
        "else:\n",
        "    merged['notional'] = np.nan\n",
        "\n",
        "merged['pnl_per_notional'] = merged['closed_pnl'] / merged['notional']\n",
        "merged['is_win'] = merged['closed_pnl'] > 0\n",
        "\n",
        "if leverage_col and leverage_col in merged.columns:\n",
        "    merged['leverage'] = pd.to_numeric(merged[leverage_col], errors='coerce')\n",
        "else:\n",
        "    merged['leverage'] = np.nan\n",
        "\n",
        "if side_col and side_col in merged.columns:\n",
        "    merged['side_norm'] = merged[side_col].astype(str).str.lower()\n",
        "else:\n",
        "    merged['side_norm'] = np.nan\n",
        "\n",
        "# Save cleaned merged\n",
        "cleaned_path = os.path.join(OUT_DIR, \"cleaned_merged_final.csv\")\n",
        "merged.to_csv(cleaned_path, index=False)\n",
        "print(\"Saved cleaned merged to:\", cleaned_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPzvy-tOEIaF",
        "outputId": "9585af82-fe77-4cc7-c01e-8552dd4984ba"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved cleaned merged to: outputs/cleaned_merged_final.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Closed PnL numeric\n",
        "if pnl_col and pnl_col in merged.columns:\n",
        "    merged['closed_pnl'] = pd.to_numeric(merged[pnl_col], errors='coerce')\n",
        "else:\n",
        "    merged['closed_pnl'] = np.nan\n",
        "    print(\"Warning: closed PnL column not found; 'closed_pnl' set to NaN. If you have a PnL column, rename it to one of:\",\n",
        "          \"closed_pnl, closedpnl, pnl, profit, realized_pnl, realised_pnl\")\n",
        "\n",
        "# Notional exposure (prefer size_usd)\n",
        "if 'size_usd' in merged.columns and merged['size_usd'].notna().sum() > 0:\n",
        "    merged['notional'] = pd.to_numeric(merged['size_usd'], errors='coerce')\n",
        "elif size_col and price_col and size_col in merged.columns and price_col in merged.columns:\n",
        "    merged['notional'] = pd.to_numeric(merged[size_col], errors='coerce') * pd.to_numeric(merged[price_col], errors='coerce')\n",
        "else:\n",
        "    merged['notional'] = np.nan\n",
        "\n",
        "merged['pnl_per_notional'] = merged['closed_pnl'] / merged['notional']\n",
        "merged['is_win'] = merged['closed_pnl'] > 0\n",
        "\n",
        "if leverage_col and leverage_col in merged.columns:\n",
        "    merged['leverage'] = pd.to_numeric(merged[leverage_col], errors='coerce')\n",
        "else:\n",
        "    merged['leverage'] = np.nan\n",
        "\n",
        "if side_col and side_col in merged.columns:\n",
        "    merged['side_norm'] = merged[side_col].astype(str).str.lower()\n",
        "else:\n",
        "    merged['side_norm'] = np.nan\n",
        "\n",
        "# Save cleaned merged\n",
        "cleaned_path = os.path.join(OUT_DIR, \"cleaned_merged_final.csv\")\n",
        "merged.to_csv(cleaned_path, index=False)\n",
        "print(\"Saved cleaned merged to:\", cleaned_path)\n",
        "\n",
        "# ----------------------------\n",
        "# 6) Exploratory Data Analysis (plots + tables)\n",
        "# ----------------------------\n",
        "print(\"\\n=== EDA Summary ===\")\n",
        "total_trades = len(merged)\n",
        "pnl_nonnull = merged['closed_pnl'].notna().sum()\n",
        "unique_accounts = merged[account_col].nunique() if account_col in merged.columns else None\n",
        "unique_symbols = merged[symbol_col].nunique() if symbol_col in merged.columns else None\n",
        "print(\"Total trades:\", total_trades)\n",
        "print(\"Trades with closed_pnl:\", pnl_nonnull)\n",
        "print(\"Unique accounts:\", unique_accounts)\n",
        "print(\"Unique symbols:\", unique_symbols)\n",
        "\n",
        "# Descriptive stats\n",
        "desc = merged[['closed_pnl','notional','pnl_per_notional','leverage']].describe()\n",
        "desc.to_csv(os.path.join(OUT_DIR, \"descriptive_stats.csv\"))\n",
        "print(\"Saved descriptive stats.\")\n",
        "\n",
        "# ---- Plot: daily closed PnL time series (and FG index if present) ----\n",
        "daily = merged.groupby('trade_date').agg(\n",
        "    daily_closed_pnl=('closed_pnl','sum'),\n",
        "    trade_count=('closed_pnl','count')\n",
        ").reset_index()\n",
        "\n",
        "if 'fg_index' in merged.columns:\n",
        "    fg_by_date = merged[['fg_date','fg_index']].drop_duplicates().dropna(subset=['fg_date']).sort_values('fg_date')\n",
        "    daily = daily.merge(fg_by_date, left_on='trade_date', right_on='fg_date', how='left')\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(pd.to_datetime(daily['trade_date']), daily['daily_closed_pnl'])\n",
        "plt.title(\"Daily aggregated closed PnL\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Daily closed PnL (sum)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR,\"daily_closed_pnl_timeseries.png\"))\n",
        "plt.close()\n",
        "\n",
        "# If FG index exists, overlay on second axis\n",
        "if 'fg_index' in daily.columns:\n",
        "    fig, ax1 = plt.subplots(figsize=(12,4))\n",
        "    ax1.plot(pd.to_datetime(daily['trade_date']), daily['daily_closed_pnl'], label='Daily PnL')\n",
        "    ax1.set_xlabel('Date')\n",
        "    ax1.set_ylabel('Daily PnL')\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(pd.to_datetime(daily['trade_date']), daily['fg_index'], linestyle='--', label='FG index')\n",
        "    ax2.set_ylabel('Fear-Greed Index')\n",
        "    plt.title(\"Daily closed PnL and Fear-Greed Index\")\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(os.path.join(OUT_DIR,\"daily_pnl_fgindex.png\"))\n",
        "    plt.close()\n",
        "\n",
        "# ---- Histogram of per-trade closed_pnl ----\n",
        "plt.figure(figsize=(8,4))\n",
        "merged['closed_pnl'].dropna().hist(bins=100)\n",
        "plt.title(\"Histogram of closed_pnl (per trade)\")\n",
        "plt.xlabel(\"closed_pnl\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR,\"hist_closed_pnl.png\"))\n",
        "plt.close()\n",
        "\n",
        "# ---- Boxplot: closed_pnl by sentiment classification (top categories) ----\n",
        "if 'classification' in merged.columns:\n",
        "    top_cats = merged['classification'].value_counts().head(8).index.tolist()\n",
        "    data = [merged.loc[merged['classification'] == cat, 'closed_pnl'].dropna().values for cat in top_cats]\n",
        "    # filter out empty groups\n",
        "    data_nonempty = [d for d in data if len(d) > 0]\n",
        "    labels_nonempty = [lab for lab, d in zip(top_cats, data) if len(d) > 0]\n",
        "    if len(data_nonempty) > 0:\n",
        "        plt.figure(figsize=(12,5))\n",
        "        plt.boxplot(data_nonempty, labels=labels_nonempty, showfliers=False)\n",
        "        plt.title(\"closed_pnl distribution by sentiment classification (top categories)\")\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUT_DIR,\"boxplot_pnl_by_sentiment.png\"))\n",
        "        plt.close()\n",
        "\n",
        "# ---- Heatmap: average PnL by leverage bucket vs sentiment ----\n",
        "if 'leverage' in merged.columns and 'classification' in merged.columns:\n",
        "    tmp = merged.copy()\n",
        "    tmp['lev_bucket'] = pd.cut(tmp['leverage'].fillna(-1), bins=[-1,0,2,5,10,20,1e9], labels=['NA','0-2','2-5','5-10','10-20','20+'])\n",
        "    pivot = tmp.groupby(['lev_bucket','classification'])['closed_pnl'].mean().unstack(fill_value=np.nan)\n",
        "    if pivot.shape[0] > 0 and pivot.shape[1] > 0:\n",
        "        plt.figure(figsize=(10,4))\n",
        "        plt.imshow(pivot.fillna(0).values, aspect='auto')\n",
        "        plt.title(\"Avg closed_pnl by leverage bucket Ã— sentiment\")\n",
        "        plt.xticks(range(pivot.shape[1]), pivot.columns, rotation=45, ha='right')\n",
        "        plt.yticks(range(pivot.shape[0]), pivot.index)\n",
        "        plt.colorbar()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUT_DIR,\"heatmap_lev_sentiment.png\"))\n",
        "        plt.close()\n",
        "\n",
        "# Save a sample view\n",
        "cols_for_sample = ['trade_date','parsed_time','account','coin','closed_pnl','notional','classification','fg_index']\n",
        "sample = merged[cols_for_sample].head(200)\n",
        "sample.to_csv(os.path.join(OUT_DIR,\"merged_sample_200.csv\"), index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aU0GQtnzElrS",
        "outputId": "c6404f0a-e22e-41b2-e8fd-90ad79970e3b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved cleaned merged to: outputs/cleaned_merged_final.csv\n",
            "\n",
            "=== EDA Summary ===\n",
            "Total trades: 211224\n",
            "Trades with closed_pnl: 211224\n",
            "Unique accounts: 32\n",
            "Unique symbols: 246\n",
            "Saved descriptive stats.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2741620310.py:105: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
            "  plt.boxplot(data_nonempty, labels=labels_nonempty, showfliers=False)\n",
            "/tmp/ipython-input-2741620310.py:116: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  pivot = tmp.groupby(['lev_bucket','classification'])['closed_pnl'].mean().unstack(fill_value=np.nan)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = {}\n",
        "if 'classification' in merged.columns:\n",
        "    cl = merged['classification'].astype(str).str.lower()\n",
        "    fear = merged.loc[cl.str.contains('fear', na=False), 'closed_pnl'].dropna()\n",
        "    greed = merged.loc[cl.str.contains('greed', na=False), 'closed_pnl'].dropna()\n",
        "    test_results['n_fear'] = int(len(fear))\n",
        "    test_results['n_greed'] = int(len(greed))\n",
        "    print(\"Counts: fear\", test_results['n_fear'], \"greed\", test_results['n_greed'])\n",
        "    if len(fear) >= 30 and len(greed) >= 30:\n",
        "        tstat, pval = stats.ttest_ind(fear, greed, equal_var=False, nan_policy='omit')\n",
        "        test_results['ttest'] = {'tstat': float(tstat), 'pval': float(pval)}\n",
        "        print(\"T-test Fear vs Greed: tstat={:.4f}, p={:.4g}\".format(tstat, pval))\n",
        "    else:\n",
        "        test_results['ttest'] = None\n",
        "        print(\"Not enough observations for robust t-test (need >=30 each).\")\n",
        "else:\n",
        "    print(\"No classification column for statistical test.\")\n",
        "\n",
        "with open(os.path.join(OUT_DIR, \"stat_tests.txt\"), \"w\") as f:\n",
        "    f.write(str(test_results))\n",
        "\n",
        "# ----------------------------\n",
        "# 8) Regression: effect of FG index on closed PnL (controlled)\n",
        "# ----------------------------\n",
        "REG_SAMPLE_MAX = 20000\n",
        "reg_cols = ['closed_pnl','fg_index','leverage','side_norm']\n",
        "if symbol_col and symbol_col in merged.columns:\n",
        "    reg_cols.append(symbol_col)\n",
        "if account_col and account_col in merged.columns:\n",
        "    reg_cols.append(account_col)\n",
        "\n",
        "reg_df = merged[[c for c in reg_cols if c in merged.columns]].copy().dropna(subset=['closed_pnl','fg_index'])\n",
        "print(\"Rows available for regression:\", len(reg_df))\n",
        "if len(reg_df) > REG_SAMPLE_MAX:\n",
        "    reg_df = reg_df.sample(n=REG_SAMPLE_MAX, random_state=42)\n",
        "\n",
        "if len(reg_df) >= 50:\n",
        "    # reduce dimensionality for categorical variables by keeping top categories\n",
        "    if account_col in reg_df.columns:\n",
        "        top_accounts = reg_df[account_col].value_counts().head(30).index.tolist()\n",
        "        reg_df['account2'] = reg_df[account_col].where(reg_df[account_col].isin(top_accounts), other='OTHER_ACC')\n",
        "    else:\n",
        "        reg_df['account2'] = 'NOACC'\n",
        "\n",
        "    if symbol_col in reg_df.columns:\n",
        "        top_symbols = reg_df[symbol_col].value_counts().head(20).index.tolist()\n",
        "        reg_df['symbol2'] = reg_df[symbol_col].where(reg_df[symbol_col].isin(top_symbols), other='OTHER_SYM')\n",
        "    else:\n",
        "        reg_df['symbol2'] = 'NOSYM'\n",
        "\n",
        "    formula = \"closed_pnl ~ fg_index + leverage + C(side_norm) + C(symbol2) + C(account2)\"\n",
        "    try:\n",
        "        model = smf.ols(formula=formula, data=reg_df).fit(cov_type='HC3')\n",
        "        with open(os.path.join(OUT_DIR, \"regression_summary.txt\"), \"w\") as f:\n",
        "            f.write(model.summary().as_text())\n",
        "        print(\"Regression completed. Coefficients (top):\")\n",
        "        print(model.params.head(10))\n",
        "    except Exception as e:\n",
        "        print(\"Regression error:\", e)\n",
        "else:\n",
        "    print(\"Not enough rows for regression after dropna; skipping regression.\")\n",
        "\n",
        "# ----------------------------\n",
        "# 9) Account-level aggregation & clustering\n",
        "# ----------------------------\n",
        "if account_col and account_col in merged.columns:\n",
        "    acct = merged.groupby(account_col).agg(\n",
        "        trades=('closed_pnl','count'),\n",
        "        total_pnl=('closed_pnl','sum'),\n",
        "        mean_pnl=('closed_pnl','mean'),\n",
        "        win_rate=('is_win','mean'),\n",
        "        avg_leverage=('leverage','mean'),\n",
        "        avg_notional=('notional','mean')\n",
        "    ).reset_index().fillna(0)\n",
        "    acct.to_csv(os.path.join(OUT_DIR,\"account_summary_raw.csv\"), index=False)\n",
        "    # Filter for accounts with >=10 trades to cluster\n",
        "    acct_filt = acct[acct['trades'] >= 10].copy()\n",
        "    print(\"Accounts with >=10 trades:\", len(acct_filt))\n",
        "    if len(acct_filt) >= 5:\n",
        "        features = ['trades','total_pnl','mean_pnl','win_rate','avg_leverage']\n",
        "        X = acct_filt[features].fillna(0).values\n",
        "        Xs = StandardScaler().fit_transform(X)\n",
        "        k = min(5, max(2, len(acct_filt)//20))\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "        acct_filt['cluster'] = kmeans.fit_predict(Xs)\n",
        "        acct_filt.to_csv(os.path.join(OUT_DIR,\"account_clusters.csv\"), index=False)\n",
        "        print(\"Saved account clusters to outputs.\")\n",
        "    else:\n",
        "        print(\"Not enough accounts with >=10 trades for clustering.\")\n",
        "else:\n",
        "    print(\"No account column found; skipping account-level aggregation & clustering.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x1GMv7gFEhA",
        "outputId": "70b8e2ad-5cd0-4ba0-8a69-6ecd0ef2b0ce"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts: fear 133871 greed 43251\n",
            "T-test Fear vs Greed: tstat=-4.9038, p=9.421e-07\n",
            "Rows available for regression: 184263\n",
            "Regression error: zero-size array to reduction operation maximum which has no identity\n",
            "Accounts with >=10 trades: 32\n",
            "Saved account clusters to outputs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report_lines = []\n",
        "report_lines.append(\"=== Quick Summary ===\")\n",
        "report_lines.append(f\"Total trades: {total_trades}\")\n",
        "report_lines.append(f\"Trades with closed_pnl: {pnl_nonnull}\")\n",
        "if 'classification' in merged.columns:\n",
        "    report_lines.append(\"Sentiment counts:\\n\" + merged['classification'].value_counts().to_string())\n",
        "if 'fg_index' in merged.columns:\n",
        "    report_lines.append(f\"FG index range: {merged['fg_index'].min()} to {merged['fg_index'].max()}\")\n",
        "report_lines.append(f\"Saved outputs to folder: {OUT_DIR}\")\n",
        "with open(os.path.join(OUT_DIR,\"quick_report.txt\"), \"w\") as f:\n",
        "    f.write(\"\\n\".join(report_lines))\n",
        "\n",
        "print(\"\\nPipeline finished. Check the 'outputs' folder for CSVs, PNGs and text summaries.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHV3_28zFOwF",
        "outputId": "6f348399-0df8-4373-c8e2-def44ae7e975"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pipeline finished. Check the 'outputs' folder for CSVs, PNGs and text summaries.\n"
          ]
        }
      ]
    }
  ]
}